"""
Chunking service for catalog data
"""
import hashlib
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
import logging

logger = logging.getLogger(__name__)

class CatalogChunker:
    """Service for chunking catalog data into manageable pieces for embeddings"""
    
    def __init__(self, max_chunk_tokens: int = 1500, column_batch_size: int = 20):
        self.max_chunk_tokens = max_chunk_tokens
        self.column_batch_size = column_batch_size
    
    def chunk_table_context(self, table: Dict[str, Any], dataset: Dict[str, Any]) -> Dict[str, Any]:
        """Create business context chunk for a table"""
        
        # Build comprehensive context
        context_text = f"""Table: {table.get('table_fqdn', '')}
Type: {table.get('object_type', 'TABLE')}
Description: {table.get('grain_description', 'No description available')}

Dataset Context:
- Dataset: {dataset.get('dataset_id', '')}
- Purpose: {dataset.get('description', 'No dataset description')}
- Domain: {dataset.get('business_domain') or table.get('business_domain') or 'General'}
- Owner: {dataset.get('owner_email', 'Unknown')}
- Source System: {dataset.get('source_system', 'Unknown')}
- Refresh Pattern: {dataset.get('refresh_cadence', 'Unknown')}

Table Statistics:
- Rows: {table.get('row_count_last_audit', 0):,}
- Columns: {table.get('column_count', 0)}
- Last Updated: {self._format_timestamp(table.get('last_updated_ts'))}
"""
        
        # Add profile status if available
        if table.get('column_profile_last_audit'):
            context_text += f"- Column Profile Date: {self._format_timestamp(table.get('column_profile_last_audit'))}\n"
        
        return {
            'id': None,  # Will be generated by BigQuery
            'table_fqdn': table.get('table_fqdn', ''),
            'dataset_id': dataset.get('dataset_id', ''),
            'table_id': table.get('table_id', ''),
            'object_type': table.get('object_type', 'TABLE'),
            'context_chunk': context_text,
            'business_domain': dataset.get('business_domain') or table.get('business_domain'),
            'grain_description': table.get('grain_description'),
            'row_count': table.get('row_count_last_audit'),
            'column_count': table.get('column_count'),
            'last_updated': self._convert_timestamp(table.get('last_updated_ts')),
            'catalog_version': self._convert_timestamp(table.get('last_updated_ts')),
            'catalog_hash': self._compute_hash(table),
            'sync_status': 'current'
        }
    
    def chunk_columns(self, table: Dict[str, Any], columns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Create column description chunks, batching columns to avoid huge chunks"""
        
        if not columns:
            return []
        
        chunks = []
        total_chunks = (len(columns) + self.column_batch_size - 1) // self.column_batch_size
        
        for i in range(0, len(columns), self.column_batch_size):
            chunk_index = i // self.column_batch_size + 1
            column_batch = columns[i:i+self.column_batch_size]
            
            # Build chunk text
            chunk_text = f"Table: {table.get('table_fqdn', '')} - Columns {i+1} to {i+len(column_batch)} of {len(columns)}\n\n"
            
            column_names = []
            has_pii = False
            null_counts = []
            total_rows = 0
            
            for col in column_batch:
                column_names.append(col.get('column_name', ''))
                
                # Check for PII
                if col.get('pii_flag'):
                    has_pii = True
                
                # Track null statistics
                if col.get('null_count') is not None and col.get('row_count'):
                    null_counts.append(col['null_count'])
                    total_rows = max(total_rows, col['row_count'])
                
                # Format column information
                chunk_text += self._format_column_info(col)
            
            # Calculate average null percentage
            null_percentage = None
            if null_counts and total_rows > 0:
                avg_nulls = sum(null_counts) / len(null_counts)
                null_percentage = (avg_nulls / total_rows) * 100
            
            chunks.append({
                'id': None,
                'table_fqdn': table.get('table_fqdn', ''),
                'chunk_index': chunk_index,
                'column_chunk': chunk_text,
                'column_names': column_names,
                'has_pii': has_pii,
                'null_percentage': null_percentage,
                'catalog_version': self._convert_timestamp(table.get('last_updated_ts')),
                'catalog_hash': self._compute_column_hash(column_batch),
                'sync_status': 'current'
            })
        
        return chunks
    
    def chunk_view_query(self, view: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Intelligently chunk large SQL queries"""
        
        sql = view.get('query', '')
        view_name = view.get('view_fqdn') or f"{view.get('project_id')}.{view.get('dataset_id')}.{view.get('table_id')}"
        
        if not sql:
            return []
        
        # Extract referenced tables
        tables_referenced = self._extract_table_references(sql)
        
        # Calculate complexity score
        complexity_score = self._calculate_query_complexity(sql)
        
        # For small queries, keep as single chunk
        if len(sql) < 2000:
            return [{
                'id': None,
                'view_fqdn': view_name,
                'chunk_index': 1,
                'query_chunk': f"View: {view_name}\nType: {view.get('view_type', 'STANDARD')}\n\nSQL Query:\n{sql}",
                'query_type': 'full',
                'tables_referenced': tables_referenced,
                'complexity_score': complexity_score,
                'catalog_version': self._convert_timestamp(view.get('last_updated_ts')),
                'catalog_hash': self._compute_hash({'query': sql}),
                'sync_status': 'current'
            }]
        
        # For large queries, split intelligently
        chunks = []
        sql_sections = self._split_sql_by_clauses(sql)
        
        for i, (section_type, section_content) in enumerate(sql_sections):
            chunk_text = f"View: {view_name} (Part {i+1}/{len(sql_sections)})\nSection: {section_type}\n\n{section_content}"
            
            chunks.append({
                'id': None,
                'view_fqdn': view_name,
                'chunk_index': i + 1,
                'query_chunk': chunk_text,
                'query_type': section_type.lower(),
                'tables_referenced': tables_referenced if i == 0 else None,  # Only store in first chunk
                'complexity_score': complexity_score if i == 0 else None,
                'catalog_version': self._convert_timestamp(view.get('last_updated_ts')),
                'catalog_hash': self._compute_hash({'query': section_content}),
                'sync_status': 'current'
            })
        
        return chunks
    
    def create_dataset_summary(self, dataset: Dict[str, Any], tables: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Create a summary chunk for a dataset"""
        
        # Count tables by type
        table_count = sum(1 for t in tables if t.get('object_type') == 'TABLE')
        view_count = sum(1 for t in tables if t.get('object_type') == 'VIEW')
        
        # Get total rows
        total_rows = sum(t.get('row_count_last_audit', 0) for t in tables)
        
        # Build summary
        summary_text = f"""Dataset: {dataset.get('dataset_id', '')}
Project: {dataset.get('project_id', '')}
Domain: {dataset.get('business_domain', 'General')}
Type: {dataset.get('dataset_type', 'Unknown')}

Description: {dataset.get('description', 'No description available')}

Statistics:
- Tables: {table_count}
- Views: {view_count}
- Total Rows: {total_rows:,}
- Owner: {dataset.get('owner_email', 'Unknown')}
- Source: {dataset.get('source_system', 'Unknown')}
- Refresh: {dataset.get('refresh_cadence', 'Unknown')}

Tables in this dataset:
"""
        
        # Add table list
        for table in tables[:20]:  # Limit to first 20 to avoid huge chunks
            summary_text += f"- {table.get('table_id', '')}: {table.get('grain_description', 'No description')[:100]}\n"
        
        if len(tables) > 20:
            summary_text += f"... and {len(tables) - 20} more tables\n"
        
        metadata = {
            'table_count': table_count,
            'view_count': view_count,
            'total_rows': total_rows,
            'table_list': [t.get('table_id', '') for t in tables]
        }
        
        return {
            'id': None,
            'summary_type': 'dataset_overview',
            'summary_key': dataset.get('dataset_id', ''),
            'summary_chunk': summary_text,
            'metadata': json.dumps(metadata),
            'catalog_version': self._convert_timestamp(dataset.get('last_updated_ts')),
            'sync_status': 'current'
        }
    
    def _format_column_info(self, col: Dict[str, Any]) -> str:
        """Format column information for chunking"""
        
        info = f"\n{col.get('column_name', 'unknown')} ({col.get('data_type', 'UNKNOWN')}):\n"
        
        # Add description if available
        if col.get('description'):
            info += f"  Description: {col['description']}\n"
        
        # Add nullability
        info += f"  Nullable: {col.get('is_nullable', 'YES')}\n"
        
        # Add statistics if available and column stats are enabled
        if col.get('profile_timestamp'):
            # Basic stats
            if col.get('distinct_count') is not None:
                info += f"  Distinct Values: {col['distinct_count']:,}\n"
            
            if col.get('null_count') is not None and col.get('row_count'):
                null_pct = (col['null_count'] / col['row_count']) * 100
                info += f"  Null Percentage: {null_pct:.1f}%\n"
            
            # Value ranges for numeric/date types
            if col.get('data_type') in ['INT64', 'FLOAT64', 'NUMERIC', 'DATE', 'TIMESTAMP']:
                if col.get('min_value'):
                    info += f"  Range: {col['min_value']} to {col.get('max_value', 'N/A')}\n"
                if col.get('average_value'):
                    info += f"  Average: {col['average_value']}\n"
            
            # Top values for categorical columns (if not PII)
            if not col.get('pii_flag') and col.get('top_5_values'):
                top_values = col['top_5_values'].split(',')[:5]  # Ensure max 5
                if top_values:
                    info += f"  Common Values: {', '.join(v.strip() for v in top_values)}\n"
        
        return info + "\n"
    
    def _compute_hash(self, data: Dict[str, Any]) -> str:
        """Compute hash of relevant data for change detection"""
        
        # Select fields that matter for content
        relevant_fields = {
            'table_fqdn': data.get('table_fqdn'),
            'grain_description': data.get('grain_description'),
            'business_domain': data.get('business_domain'),
            'row_count': data.get('row_count_last_audit'),
            'column_count': data.get('column_count'),
            'query': data.get('query'),  # For views
            'description': data.get('description')  # For datasets
        }
        
        # Remove None values
        relevant_fields = {k: v for k, v in relevant_fields.items() if v is not None}
        
        # Create stable string representation
        content = json.dumps(relevant_fields, sort_keys=True)
        
        return hashlib.sha256(content.encode()).hexdigest()
    
    def _compute_column_hash(self, columns: List[Dict[str, Any]]) -> str:
        """Compute hash for a batch of columns"""
        
        column_data = []
        for col in columns:
            column_data.append({
                'name': col.get('column_name'),
                'type': col.get('data_type'),
                'nullable': col.get('is_nullable'),
                'description': col.get('description'),
                'distinct_count': col.get('distinct_count')
            })
        
        content = json.dumps(column_data, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()
    
    def _format_timestamp(self, ts: Optional[int]) -> str:
        """Format Unix timestamp to readable string"""
        if not ts:
            return 'Unknown'
        
        try:
            # Convert milliseconds to seconds
            dt = datetime.fromtimestamp(ts / 1000)
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            return 'Unknown'
    
    def _convert_timestamp(self, ts: Optional[int]) -> Optional[str]:
        """Convert Unix timestamp to BigQuery timestamp string"""
        if not ts:
            return None
        
        try:
            dt = datetime.fromtimestamp(ts / 1000)
            return dt.isoformat()
        except:
            return None
    
    def _extract_table_references(self, sql: str) -> List[str]:
        """Extract table references from SQL"""
        
        # Simple regex to find table references
        # Matches: FROM table, JOIN table, from `project.dataset.table`
        pattern = r'(?:FROM|JOIN)\s+`?([a-zA-Z0-9_]+(?:\.[a-zA-Z0-9_]+)*)`?'
        
        matches = re.findall(pattern, sql, re.IGNORECASE)
        
        # Deduplicate and clean
        tables = list(set(match.strip('`') for match in matches))
        
        return tables[:10]  # Limit to 10 to avoid array size issues
    
    def _calculate_query_complexity(self, sql: str) -> int:
        """Calculate a simple complexity score for a query"""
        
        score = 0
        
        # Count major clauses
        clauses = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'HAVING', 'UNION', 'WITH']
        for clause in clauses:
            score += len(re.findall(rf'\b{clause}\b', sql, re.IGNORECASE))
        
        # Count joins
        score += len(re.findall(r'\bJOIN\b', sql, re.IGNORECASE)) * 2
        
        # Count subqueries
        score += sql.count('(SELECT') * 3
        
        # Length factor
        score += len(sql) // 1000
        
        return min(score, 100)  # Cap at 100
    
    def _split_sql_by_clauses(self, sql: str) -> List[Tuple[str, str]]:
        """Split SQL into logical sections"""
        
        sections = []
        
        # Define major SQL sections
        section_patterns = [
            (r'WITH\s+.*?(?=SELECT)', 'WITH_CLAUSE'),
            (r'SELECT\s+.*?(?=FROM)', 'SELECT_CLAUSE'),
            (r'FROM\s+.*?(?=WHERE|GROUP BY|ORDER BY|LIMIT|$)', 'FROM_CLAUSE'),
            (r'WHERE\s+.*?(?=GROUP BY|ORDER BY|LIMIT|$)', 'WHERE_CLAUSE'),
            (r'GROUP BY\s+.*?(?=ORDER BY|LIMIT|$)', 'GROUP_BY'),
            (r'ORDER BY\s+.*?(?=LIMIT|$)', 'ORDER_BY')
        ]
        
        remaining_sql = sql
        
        for pattern, section_type in section_patterns:
            match = re.search(pattern, remaining_sql, re.IGNORECASE | re.DOTALL)
            if match:
                section_content = match.group(0).strip()
                if len(section_content) > 100:  # Only create section if substantial
                    sections.append((section_type, section_content))
        
        # If no sections found or SQL is still simple, return as single chunk
        if not sections:
            sections = [('FULL_QUERY', sql)]
        
        return sections